{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GIHHManyFeatures",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNqSVJZFUR-L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "f1b7e1d9-fc78-4707-a12a-321a73bbf3fe"
      },
      "source": [
        "# Data Manipulation Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "# Vizualization Libraries\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# pre-processing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ML model Libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#statsmodels\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import acf\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.arima_model import ARMA\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from statsmodels.tsa.arima_process import ArmaProcess\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "!pip install pmdarima\n",
        "from pmdarima.arima import auto_arima\n",
        "\n",
        "## for Deep-learing:\n",
        "import keras\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import SGD \n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import np_utils\n",
        "import itertools\n",
        "from keras.layers import LSTM, RepeatVector, TimeDistributed, Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning:\n",
            "\n",
            "pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting pmdarima\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/62/725b3b6ae0e56c77534de5a8139322e7b863ca53fd5bd6bd3b7de87d0c20/pmdarima-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (1.1.4)\n",
            "Collecting statsmodels<0.12,>=0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/83/540fd83238a18abe6c2d280fa8e489ac5fcefa1f370f0ca1acd16ae1b860/statsmodels-0.11.1-cp36-cp36m-manylinux1_x86_64.whl (8.7MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7MB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (0.22.2.post1)\n",
            "Collecting setuptools<50.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/a9/5dc32465951cf4812e9e93b4ad2d314893c2fa6d5f66ce5c057af6e76d85/setuptools-49.6.0-py3-none-any.whl (803kB)\n",
            "\u001b[K     |████████████████████████████████| 808kB 37.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (0.17.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (1.4.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (1.24.3)\n",
            "Collecting Cython<0.29.18,>=0.29\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/d7/510ddef0248f3e1e91f9cc7e31c0f35f8954d0af92c5c3fd4c853e859ebe/Cython-0.29.17-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 45.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.6/dist-packages (from pmdarima) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pmdarima) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pmdarima) (2018.9)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.6/dist-packages (from statsmodels<0.12,>=0.11->pmdarima) (0.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.19->pmdarima) (1.15.0)\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: statsmodels, setuptools, Cython, pmdarima\n",
            "  Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "  Found existing installation: setuptools 50.3.2\n",
            "    Uninstalling setuptools-50.3.2:\n",
            "      Successfully uninstalled setuptools-50.3.2\n",
            "  Found existing installation: Cython 0.29.21\n",
            "    Uninstalling Cython-0.29.21:\n",
            "      Successfully uninstalled Cython-0.29.21\n",
            "Successfully installed Cython-0.29.17 pmdarima-1.7.1 setuptools-49.6.0 statsmodels-0.11.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources",
                  "statsmodels"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51KJCJ2wU7AB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "880428c0-1021-4634-c365-e4c864670261"
      },
      "source": [
        "retail = pd.read_csv('/content/drive/My Drive/TheGreatIndianHiringHackathon/Train.csv')\n",
        "retail_test = pd.read_csv('/content/drive/My Drive/TheGreatIndianHiringHackathon/Test.csv')\n",
        "retail_data = retail.copy()\n",
        "retail_test_data = retail_test.copy()\n",
        "\n",
        "#Drop Duplicate rows\n",
        "retail_data.drop_duplicates(subset=None, keep='first', inplace=True)\n",
        "#Only dropped one outlier\n",
        "retail_data.drop(retail_data.loc[retail_data['UnitPrice']>35000,:].index,inplace=True)\n",
        "\n",
        "# #No missing values\n",
        "# import missingno as msno\n",
        "# msno.matrix(retail_test)\n",
        "# print(retail_test.isna().sum())\n",
        "#Seperate Categorical and Numerical Columns\n",
        "cat_cols = retail_data.select_dtypes(include=['object','category']).columns.tolist()\n",
        "print(cat_cols)\n",
        "\n",
        "num_cols = retail_data.select_dtypes(include=['int64','float64']).columns.tolist()\n",
        "print(num_cols)\n",
        "\n",
        "retail_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['InvoiceDate']\n",
            "['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'UnitPrice', 'CustomerID', 'Country']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>InvoiceNo</th>\n",
              "      <th>StockCode</th>\n",
              "      <th>Description</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>InvoiceDate</th>\n",
              "      <th>UnitPrice</th>\n",
              "      <th>CustomerID</th>\n",
              "      <th>Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6141</td>\n",
              "      <td>1583</td>\n",
              "      <td>144</td>\n",
              "      <td>3</td>\n",
              "      <td>2011-05-06 16:54:00</td>\n",
              "      <td>3.75</td>\n",
              "      <td>14056.0</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6349</td>\n",
              "      <td>1300</td>\n",
              "      <td>3682</td>\n",
              "      <td>6</td>\n",
              "      <td>2011-05-11 07:35:00</td>\n",
              "      <td>1.95</td>\n",
              "      <td>13098.0</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16783</td>\n",
              "      <td>2178</td>\n",
              "      <td>1939</td>\n",
              "      <td>4</td>\n",
              "      <td>2011-11-20 13:20:00</td>\n",
              "      <td>5.95</td>\n",
              "      <td>15044.0</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16971</td>\n",
              "      <td>2115</td>\n",
              "      <td>2983</td>\n",
              "      <td>1</td>\n",
              "      <td>2011-11-22 12:07:00</td>\n",
              "      <td>0.83</td>\n",
              "      <td>15525.0</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6080</td>\n",
              "      <td>1210</td>\n",
              "      <td>2886</td>\n",
              "      <td>12</td>\n",
              "      <td>2011-05-06 09:00:00</td>\n",
              "      <td>1.65</td>\n",
              "      <td>13952.0</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   InvoiceNo  StockCode  Description  ...  UnitPrice CustomerID  Country\n",
              "0       6141       1583          144  ...       3.75    14056.0       35\n",
              "1       6349       1300         3682  ...       1.95    13098.0       35\n",
              "2      16783       2178         1939  ...       5.95    15044.0       35\n",
              "3      16971       2115         2983  ...       0.83    15525.0       35\n",
              "4       6080       1210         2886  ...       1.65    13952.0       35\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoovT5do2BnL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b1dd111-0127-40c2-8951-93bc4131e473"
      },
      "source": [
        "retail_test_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(122049, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtBEsphQdSN6"
      },
      "source": [
        "def drop_irrelavant_columns(df):\n",
        "  df.drop(columns=['InvoiceNo','Description','Quantity','CustomerID','Country'],inplace=True)\n",
        "\n",
        "def sample_by_hour_set_index(df):\n",
        "  df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
        "  df.set_index('InvoiceDate',inplace=True)\n",
        "  df.index = pd.to_datetime(df.index.strftime('%Y-%m-%d %H:%M:%S'))\n",
        "  df.sort_index(inplace=True)\n",
        "  df.resample(rule='H').mean().fillna(method ='ffill', inplace = True)\n",
        "\n",
        "def convert_InvoiceDate_to_features(df):\n",
        "  df['year'] = df.index.year\n",
        "  df['month'] = df.index.month\n",
        "  df['day'] = df.index.day\n",
        "  df['hour'] = df.index.hour\n",
        "  df['minute'] = df.index.minute\n",
        "  return df\n",
        "\n",
        "\n",
        "def convert_column_to_dummies(df,colname):\n",
        "  dummies = pd.get_dummies(df[colname])\n",
        "\n",
        "  for col in dummies.columns:\n",
        "    df[col] = dummies[col]\n",
        "\n",
        "  df.drop(columns=[colname],inplace=True)\n",
        "\n",
        "def convert_unit_price_to_supervised(df):\n",
        "  reframed = series_to_supervised(df['UnitPrice'].values, n_in=7, n_out=1, dropnan=True)\n",
        "  df[reframed.columns] = reframed.values\n",
        "  return df\n",
        "\n",
        "def series_to_supervised(data, n_in=7, n_out=1, dropnan=True):\n",
        "\tn_vars = 1\n",
        "\tdff = pd.DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(dff.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\t# for i in range(0, n_out):\n",
        "\t# \tcols.append(dff.shift(-i))\n",
        "\t# \tif i == 0:\n",
        "\t# \t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "\t# \telse:\n",
        "\t# \t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# put it all together\n",
        "\tagg = pd.concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\t# drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.fillna(method='bfill', inplace=True)\n",
        "\treturn agg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP2hw5FbNMuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "677eff1d-5cc9-40d6-efc3-e462b20bd907"
      },
      "source": [
        "retail_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(282158, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lny3p5eBBNI-"
      },
      "source": [
        "drop_irrelavant_columns(retail_data)\n",
        "drop_irrelavant_columns(retail_test_data)\n",
        "\n",
        "sample_by_hour_set_index(retail_data)\n",
        "sample_by_hour_set_index(retail_test_data)\n",
        "\n",
        "retail_data = convert_InvoiceDate_to_features(retail_data)\n",
        "retail_test_data = convert_InvoiceDate_to_features(retail_test_data)\n",
        "\n",
        "convert_column_to_dummies(retail_data,'StockCode')\n",
        "convert_column_to_dummies(retail_test_data,'StockCode')\n",
        "\n",
        "model_ready_data = convert_unit_price_to_supervised(retail_data)\n",
        "\n",
        "model_ready_data.head()\n",
        "model_ready_data_small = model_ready_data[:150000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib578pYsEcqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f274b093-b985-4458-aae2-d298d97af9c8"
      },
      "source": [
        "# split into train and test sets\n",
        "values = model_ready_data_small.values\n",
        "\n",
        "n_train_time = int(len(model_ready_data_small)*0.75)\n",
        "train = values[:n_train_time, :]\n",
        "test = values[n_train_time:, :]\n",
        "##test = values[n_train_time:n_test_time, :]\n",
        "# split into input and outputs\n",
        "train_X, train_y = train[:, :-1], train[:, -1]\n",
        "test_X, test_y = test[:, :-1], test[:, -1]\n",
        "# reshape input to be 3D [samples, timesteps, features]\n",
        "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
        "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
        "\n",
        "test_X = train_X\n",
        "test_y = train_y\n",
        "\n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(112500, 1, 3631) (112500,) (112500, 1, 3631) (112500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy-Vz-haErkn"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(5, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "#    model.add(LSTM(70))\n",
        "#    model.add(Dropout(0.3))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "#CNN + LSTM\n",
        "# model_cnn_lstm = Sequential()\n",
        "# model_cnn_lstm.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None,train_X.shape[1], train_X.shape[2])))\n",
        "# model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=1)))\n",
        "# model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
        "# model_cnn_lstm.add(LSTM(50, activation='relu'))\n",
        "# model_cnn_lstm.add(Dense(1))\n",
        "# model_cnn_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# fit network\n",
        "history = model.fit(train_X, train_y, epochs=20, batch_size=70, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# # make a prediction\n",
        "# yhat = model.predict(test_X)\n",
        "# test_X = test_X.reshape((test_X.shape[0], test_X.shape[]))\n",
        "# # invert scaling for forecast\n",
        "# inv_yhat = np.concatenate((yhat, test_X[:, -7:]), axis=1)\n",
        "# # inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "# inv_yhat = inv_yhat[:,0]\n",
        "# # invert scaling for actual\n",
        "# test_y = test_y.reshape((len(test_y), 1))\n",
        "# inv_y = np.concatenate((test_y, test_X[:, -7:]), axis=1)\n",
        "# # inv_y = scaler.inverse_transform(inv_y)\n",
        "# inv_y = inv_y[:,0]\n",
        "# # calculate RMSE\n",
        "# rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "# print('Test RMSE: %.3f' % rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvHLjIUFJ4Ot"
      },
      "source": [
        "result = pd.DataFrame(inv_yhat,index=x_train.index[:-7])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DVXKVIaZjfH"
      },
      "source": [
        "a = []\n",
        "while(1):\n",
        "    a.append('1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bejxcl8EKc1B"
      },
      "source": [
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PIx5OEqIoEI"
      },
      "source": [
        "ax = plt.plot(inv_y)\n",
        "ax = plt.plot(inv_yhat)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On2X8K8oS6tT"
      },
      "source": [
        "match_test_data = retail_test_data.index\n",
        "hour_converted = []\n",
        "for i in match_test_data:\n",
        "  hour_converted.append(i.strftime('%Y-%m-%d %H:00:00'))\n",
        "\n",
        "new_result = result\n",
        "new_result['map'] = new_result.index.strftime('%Y-%m-%d %H:00:00')\n",
        "new_result.columns = ['c','map']\n",
        "new_result.index = new_result['map']\n",
        "replace_dict = new_result['c'].to_dict()\n",
        "retail_test_data['predictions'] = retail_test_data.index.strftime('%Y-%m-%d %H:00:00')\n",
        "retail_test_data['predictions'].replace(replace_dict,inplace=True)\n",
        "future_predictions = {'2011-12-09 08:00:00':2.23021,'2011-12-09 09:00:00':2.23021,'2011-12-09 10:00:00':2.23021,'2011-12-09 11:00:00':2.23021,'2011-12-09 12:00:00':2.23021}\n",
        "retail_test_data['predictions'].replace(future_predictions,inplace=True)\n",
        "\n",
        "\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RzLmU_SmTJi"
      },
      "source": [
        " retail_test_data['predictions'] = np.round(retail_test_data['predictions'].astype(float),decimals=2)\n",
        "retail_test_data.sort_values('order',inplace=True)\n",
        "UnitPrice = pd.DataFrame(retail_test_data['predictions'].astype('float').values,columns=['UnitPrice'])\n",
        "UnitPrice.to_csv('/content/drive/My Drive/TheGreatIndianHiringHackathon/'+'kkcnnlstm05'+'.csv', index=False, index_label=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlXDforbc4F_"
      },
      "source": [
        "fg = pd.read_csv('/content/kklstm01.csv')\n",
        "fg.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz43iqRC0OOO"
      },
      "source": [
        "c = retail_data.sort_index()\n",
        "c.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMQ2THux0ehU"
      },
      "source": [
        "h = retail_test_data.sort_index()\n",
        "h.tail(40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ys2vDCesodw"
      },
      "source": [
        "# #SARIMAX\n",
        "# model = SARIMAX(y_train, order=(3,1,0),seasonal_order=(1,0,0,7), seasonal=True, enforce_invertibility=False, enforce_stationarity=False,exog=x_train)\n",
        "# sarimax_results = model.fit()\n",
        "# predictions = sarimax_results.predict(start=x_train.index[0], end=x_train.index[-1],exog=x_test)\n",
        "# plt.figure(figsize=(20,5))\n",
        "# ax = plt.plot(y_train)\n",
        "# ax = plt.plot(predictions)\n",
        "# plt.show()\n",
        "# #One thing that could be done is by distributing the day prediction accross the time stamps of that day. could reduce the error.\n",
        "# mean_squared_error(y_train,predictions)\n",
        "# # mean_prediction=predictions\n",
        "# # mean_prediction.loc[:] = np.round(y_train.mean(),decimals=2)\n",
        "# # match_predictions(mean_prediction,retail_test_data,'kkmean10')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}